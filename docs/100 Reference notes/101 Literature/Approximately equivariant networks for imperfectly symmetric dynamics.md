---
authors:
  - "[[Rui Wang|Rui Wang]]"
  - "[[Robin Walters|Robin Walters]]"
  - "[[Rose Yu|Rose Yu]]"
year: 2022
tags:
  - relaxed_equivariance
  - equivariance
  - dl_theory
  - paper
url: https://proceedings.mlr.press/v162/wang22aa.html
share: true
---
> [!info] Abstract
> Incorporating symmetry as an inductive bias into neural network architecture has led to improvements in generalization, data efficiency, and physical consistency in dynamics modeling. Methods such as CNNs or equivariant neural networks use weight tying to enforce symmetries such as shift invariance or rotational equivariance. However, despite the fact that physical laws obey many symmetries, real-world dynamical data rarely conforms to strict mathematical symmetry either due to noisy or incomplete data or to symmetry breaking features in the underlying dynamical system. We explore approximately equivariant networks which are biased towards preserving symmetry but are not strictly constrained to do so. By relaxing equivariance constraints, we find that our models can outperform both baselines with no symmetry bias and baselines with overly strict symmetry in both simulated turbulence domains and real-world multi-stream jet flow.
