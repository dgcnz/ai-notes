---
tags:
  - quantization
  - efficient_dl
share: true
---
Related:
- [HuggingFace Docs](https://huggingface.co/docs/transformers/en/main_classes/quantization)
- [[A survey of quantization methods for efficient neural network inference|A survey of quantization methods for efficient neural network inference]]
- A recent (2024) work by Han et al: [[AWQ - Activation-aware Weight Quantization for LLM Compression and Acceleration|AWQ - Activation-aware Weight Quantization for LLM Compression and Acceleration]]